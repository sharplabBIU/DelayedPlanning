{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed6614b",
   "metadata": {},
   "source": [
    "### 1 Limited‑breadth Monte‑Carlo planner\n",
    "\n",
    "For each goal depth $g\\in\\{1,2,3\\}$, decision index $d\\in\\{1,2,3\\}$ and state $s$ we pre‑tabulate two discovery probabilities $\\bigl(p^{\\text{base}}_{g,d,s},\\,p^{\\text{base2}}_{g,d,s}\\bigr)$ indicating whether **one** or **two** goal‑leading actions exist (non‑zero entries are listed in Table S1).\n",
    "\n",
    "The planner’s effective breadth—i.e. the expected number of roll‑outs per node—is\n",
    "\n",
    "$$\n",
    "b = 9\\,\\sigma(\\tilde b),\\qquad \n",
    "b_2 = 5\\,\\sigma(\\tilde b_2) \\tag{1a}\n",
    "$$\n",
    "\n",
    "with the logistic transform $\\sigma(x)=(1+e^{-x})^{-1}$.\n",
    "\n",
    "Experience counters accumulate deterministically:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E^{(1)}_{t,d,s} &= E^{(1)}_{t-1,d,s} + b\\,\\mathbf 1\\!\\bigl[p^{\\text{base}}_{g,d,s}>0\\bigr],\\\\\n",
    "E^{(2)}_{t,d,s} &= E^{(2)}_{t-1,d,s} + b_2\\,\\mathbf 1\\!\\bigl[p^{\\text{base2}}_{g,d,s}>0\\bigr].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The resulting probabilities of discovering two, one or zero goal‑leading actions are\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p^{(2)}_{t,d,s} &= \\min\\!\\bigl(p^{\\text{base2}}_{g,d,s}\\,E^{(2)}_{t,d,s},1\\bigr),\\\\\n",
    "p^{(1)}_{t,d,s} &= \\min\\!\\bigl(p^{\\text{base}}_{g,d,s}\\,E^{(1)}_{t,d,s},1\\bigr),\\\\\n",
    "p^{(0)}_{t,d,s} &= 1 - p^{(1)}_{t,d,s}.\n",
    "\\end{aligned} \\tag{1}\n",
    "$$\n",
    "\n",
    "*Interpretation.*  Larger $\\tilde b$ (or $\\tilde b_2$) steepens the linear growth in (1a), accelerating discovery; once $p^{(1)}$ or $p^{(2)}$ saturates at 1, extra sampling no longer helps.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 Instrumental state‑action values\n",
    "\n",
    "Let $R_g=4$ denote the reward for reaching the goal and $\\gamma=\\sigma(\\tilde\\gamma)$ an individual discount factor.  Taking control at decision $d$ (for goal depth $g$) yields\n",
    "\n",
    "$$\n",
    "V_{\\text{take}} = R_g\\,\\gamma^{\\Delta},\n",
    "\\qquad \n",
    "V_{\\text{relinquish}} = 1,\n",
    "\\quad\n",
    "\\Delta = g-d \\tag{2}\n",
    "$$\n",
    "\n",
    "If two goal paths are discovered for $g\\in\\{1,2\\}$, relinquishing also delivers the discounted reward $V_{\\text{relinquish}} = 1 + R_g\\,\\gamma^{\\Delta}$.  The constant 1 captures an exogenous incentive to cede control when both options are equal.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 Soft‑max choice with bias and caching\n",
    "\n",
    "Effective action values combine model‑based assessments, a fixed bias, and a cached habitual tendency:\n",
    "\n",
    "$$\n",
    "Q^{(n)}_{t,d,s} =\n",
    "w_{\\text{MB}}\\,\\mathbf V^{(n)}_{t,d,s}\n",
    "+ [\\beta_{\\text{bias}},\\,0]\n",
    "+ \\beta_{\\text{cache}}\\,\\mathbf C_{t,d,s}. \\tag{3}\n",
    "$$\n",
    "\n",
    "After trial outcome $o_t\\in\\{-1,+1\\}$ the cache updates only for the realised meta‑action $a_{t,d}$:\n",
    "\n",
    "$$\n",
    "\\alpha_t =\n",
    "\\sigma\\!\\bigl(\\tilde\\alpha + o_t\\,[\\beta_{\\text{rel.}},\\,\\beta_{\\text{ctrl}}]_{a_{t,d}}\\bigr), \\qquad\n",
    "\\mathbf C_{t+1,d,s} =\n",
    "\\mathbf C_{t,d,s} + \\alpha_t\\bigl(\\mathbf e_{a_{t,d}} - \\mathbf C_{t,d,s}\\bigr). \\tag{4}\n",
    "$$\n",
    "\n",
    "A soft‑max over $Q^{(n)}_{t,d,s}$ yields policy $\\pi^{(n)}_{t,d,s}(a)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 Mixture likelihood\n",
    "\n",
    "Because the planning outcome $n\\in\\{0,1,2\\}$ is latent, the observed meta‑action probability is\n",
    "\n",
    "$$\n",
    "P\\!\\bigl(a_{t,d}\\mid\\theta\\bigr) \\;=\\;\n",
    "\\sum_{n=0}^{2} p^{(n)}_{t,d,s}\\;\n",
    "\\pi^{(n)}_{t,d,s}\\!\\bigl(a_{t,d}\\bigr). \\tag{6}\n",
    "$$\n",
    "\n",
    "Summing the log of (6) over all meta‑decisions yields the total log‑likelihood\n",
    "\n",
    "$$\n",
    "\\log\\mathcal L(\\theta) \\;=\\;\n",
    "\\sum_{t,d}\\;\n",
    "\\log\\Bigl[\n",
    "\\sum_{n}\n",
    "p^{(n)}_{t,d,s}\\;\n",
    "\\pi^{(n)}_{t,d,s}\\!\\bigl(a_{t,d}\\bigr)\n",
    "\\Bigr]. \\tag{7}\n",
    "$$\n",
    "\n",
    "Parameters $\\theta=\\{\\tilde b,\\tilde b_2,\\tilde\\gamma,\\tilde w,\n",
    "\\beta_{\\text{bias}},\\beta_{\\text{cache}},\n",
    "\\tilde\\alpha,\\beta_{\\text{rel.}},\\beta_{\\text{ctrl}}\\}$\n",
    "are estimated by maximising (7) or via hierarchical Bayesian inference.\n",
    "Equations (1a)–(7) reproduce exactly the implementation\n",
    "`ChoiceBias_MB_Breadth_Depth_actionSeparation_Cached`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25a6a2-2c67-4081-9b4f-5f9755ce5cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
